# -*- coding: utf-8 -*-
"""Skin_Lesion_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1po7e3_6DeXLAfV9UVd7gJ3Ho-aQ28MW4
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import PIL
from PIL import Image
import cv2
import os
import matplotlib.pyplot as plt
#from google.colab.patches import cv2_imshow
# %matplotlib inline

ground_truth = pd.read_csv('~/project/ISIC_2019_Training_GroundTruth.csv')
ground_truth.tail()

output_dir = "ISIC_2019_Augmented_Input"

import os

path = output_dir
num_images = sum([len(files) for r, d, files in os.walk(path)])
print("Number of images:", num_images)

# Initialize an empty DataFrame to store the augmented image IDs and labels
augmented_data_df = pd.DataFrame(columns=ground_truth.columns)

# List all files in the input directory
image_files = os.listdir(output_dir)
for file in image_files:
  if file.find("jpg") != -1:
   # Get the label for the image
    image_identifier = file[0:file.find("augmented")-1]
    #image_identifier = file[0:file.find("jpg")-1]
    specific_image_row = ground_truth[ground_truth['image'] == image_identifier]
    label_data = specific_image_row.drop(columns=['image']).iloc[0].to_dict()

  # Append to the DataFrame
    new_row = {"image": file}
    new_row.update(label_data)
    new_data = pd.DataFrame([new_row])
    augmented_data_df = pd.concat([augmented_data_df, new_data], ignore_index=True)

augmented_data_df = augmented_data_df.sort_values(by=['image'])
augmented_data_df

mel_df = augmented_data_df.groupby("MEL").get_group(1)
nv_df = augmented_data_df.groupby("NV").get_group(1)
bcc_df = augmented_data_df.groupby("BCC").get_group(1)
ak_df = augmented_data_df.groupby("AK").get_group(1)
bkl_df = augmented_data_df.groupby("BKL").get_group(1)
df_df = augmented_data_df.groupby("DF").get_group(1)
vasc_df = augmented_data_df.groupby("VASC").get_group(1)
scc_df = augmented_data_df.groupby("SCC").get_group(1)

num_rows1 = mel_df.shape[0]
last_20_percent1 = int(num_rows1 * 0.2)
first_70_percent1 = int(num_rows1 * 0.7)

num_rows2 = nv_df.shape[0]
last_20_percent2 = int(num_rows2 * 0.2)
first_70_percent2 = int(num_rows2 * 0.7)

num_rows3 = bcc_df.shape[0]
last_20_percent3 = int(num_rows3 * 0.2)
first_70_percent3 = int(num_rows3 * 0.7)

num_rows4 = ak_df.shape[0]
last_20_percent4 = int(num_rows4 * 0.2)
first_70_percent4 = int(num_rows4 * 0.7)

num_rows5 = bkl_df.shape[0]
last_20_percent5 = int(num_rows5 * 0.2)
first_70_percent5 = int(num_rows5 * 0.7)

num_rows6 = df_df.shape[0]
last_20_percent6 = int(num_rows6 * 0.2)
first_70_percent6 = int(num_rows6 * 0.7)

num_rows7 = vasc_df.shape[0]
last_20_percent7 = int(num_rows7 * 0.2)
first_70_percent7 = int(num_rows7 * 0.7)

num_rows8 = scc_df.shape[0]
last_20_percent8 = int(num_rows8 * 0.2)
first_70_percent8 = int(num_rows8 * 0.7)

last_20_percent_df1 = mel_df.iloc[-last_20_percent1:]
last_20_percent_df2 = nv_df.iloc[-last_20_percent2:]
last_20_percent_df3 = bcc_df.iloc[-last_20_percent3:]
last_20_percent_df4 = ak_df.iloc[-last_20_percent4:]
last_20_percent_df5 = bkl_df.iloc[-last_20_percent5:]
last_20_percent_df6 = df_df.iloc[-last_20_percent6:]
last_20_percent_df7 = vasc_df.iloc[-last_20_percent7:]
last_20_percent_df8 = scc_df.iloc[-last_20_percent8:]

first_70_percent_df1 = mel_df.iloc[:first_70_percent1]
first_70_percent_df2 = nv_df.iloc[:first_70_percent2]
first_70_percent_df3 = bcc_df.iloc[:first_70_percent3]
first_70_percent_df4 = ak_df.iloc[:first_70_percent4]
first_70_percent_df5 = bkl_df.iloc[:first_70_percent5]
first_70_percent_df6 = df_df.iloc[:first_70_percent6]
first_70_percent_df7 = vasc_df.iloc[:first_70_percent7]
first_70_percent_df8 = scc_df.iloc[:first_70_percent8]

test_1 = mel_df.iloc[first_70_percent1:-last_20_percent1]
test_2 = nv_df.iloc[first_70_percent2:-last_20_percent2]
test_3 = bcc_df.iloc[first_70_percent3:-last_20_percent3]
test_4 = ak_df.iloc[first_70_percent4:-last_20_percent4]
test_5 = bkl_df.iloc[first_70_percent5:-last_20_percent5]
test_6 = df_df.iloc[first_70_percent6:-last_20_percent6]
test_7 = vasc_df.iloc[first_70_percent7:-last_20_percent7]
test_8 = scc_df.iloc[first_70_percent8:-last_20_percent8]

augmented_data_df = pd.concat([first_70_percent_df1, first_70_percent_df2, first_70_percent_df3, first_70_percent_df4, first_70_percent_df5, first_70_percent_df6, first_70_percent_df7, first_70_percent_df8])
augmented_test_df = pd.concat([last_20_percent_df1, last_20_percent_df2, last_20_percent_df3, last_20_percent_df4, last_20_percent_df5, last_20_percent_df6, last_20_percent_df7, last_20_percent_df8])
augmented_validation_df = pd.concat([test_1, test_2, test_3, test_4, test_5, test_6, test_7, test_8])

augmented_data_df = augmented_data_df.drop('UNK', axis=1)
augmented_validation_df = augmented_validation_df.drop('UNK', axis=1)
augmented_test_df = augmented_test_df.drop('UNK', axis=1)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)

train_generator = datagen.flow_from_dataframe(
    dataframe=augmented_data_df,
    directory= output_dir,
    x_col="image",
    y_col=augmented_data_df.columns[1:],
    batch_size=16,
    shuffle=True,
    class_mode="raw",
    target_size=(224,224))

valid_generator = datagen.flow_from_dataframe(
    dataframe=augmented_validation_df,
    directory= output_dir,
    x_col="image",
    y_col=augmented_validation_df.columns[1:],
    batch_size=16,
    shuffle=True,
    class_mode="raw",
    target_size=(224,224))

test_generator = datagen.flow_from_dataframe(
    dataframe=augmented_test_df,
    directory= output_dir,
    x_col="image",
    y_col=augmented_validation_df.columns[1:],
    batch_size=16,
    shuffle=False,
    class_mode="raw",
    target_size=(224,224))

combined_data_df = pd.concat([augmented_data_df, augmented_validation_df])
combined_generator = datagen.flow_from_dataframe(
    dataframe=combined_data_df,
    directory= output_dir,
    x_col="image",
    y_col=combined_data_df.columns[1:],
    batch_size=16,
    shuffle=True,
    class_mode="raw",
    target_size=(224,224))

from tensorflow.keras.optimizers import RMSprop
import tensorflow as tf

# Load the best model for testing
best_model = tf.keras.models.load_model("resnet_rmsprop_noaug.h5")

# Retrain the model using both training and validation data
best_model.compile(optimizer=RMSprop(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
# Calculate test loss and accuracy based on the final averaged predictions

import numpy as np
from tensorflow.keras.preprocessing import image
from sklearn.metrics import accuracy_score

# Reset the generator before starting TTA
test_generator.reset()

# Number of augmentations
num_augmentations = 3

# Weight for the original image
original_image_weight = 2.0  # Adjust this value as needed

# Initialize variables to store predictions for all images
all_predictions_all_images = []

# Iterate over all batches in the test generator
for batch_index in range(len(test_generator)):
    # Get the batch of original images and labels
    batch_images, batch_labels = test_generator[batch_index]

    # Initialize variables to store predictions for a single batch
    all_predictions_single_batch = []

    # Iterate over images in the batch
    for image_index in range(len(batch_images)):
        # Get the original image
        original_image = batch_images[image_index]

        # Initialize variables to store predictions for a single image
        all_predictions_single_image = []

        # Make predictions on the original image
        o_image = np.expand_dims(original_image, axis=0)
        predictions_original = best_model.predict(o_image)
        all_predictions_single_image.append(predictions_original * original_image_weight)

        # Augment the image multiple times and make predictions
        for _ in range(num_augmentations):
            # Apply random augmentations
            augmented_image = image.random_rotation(original_image, rg=30, row_axis=0, col_axis=1, channel_axis=2)
            augmented_image = image.random_zoom(augmented_image, (0.8, 1.2))
            augmented_image = np.fliplr(augmented_image)
            augmented_image = np.flipud(augmented_image)
            augmented_image = image.random_shear(augmented_image, intensity=20, row_axis=0, col_axis=1, channel_axis=2)
            augmented_image = image.random_shift(augmented_image, wrg=0.2, hrg=0.2, row_axis=0, col_axis=1, channel_axis=2)

            # Expand dimensions to match model input shape
            augmented_image = np.expand_dims(augmented_image, axis=0)

            # Make predictions on the augmented image
            predictions_augmented = best_model.predict(augmented_image)

            # Append the predictions to the list for a single image
            all_predictions_single_image.append(predictions_augmented)

        # Average predictions across augmentations for a single image
        average_predictions_single_image = np.mean(all_predictions_single_image, axis=0)

        # Append the averaged predictions for a single image to the list for a single batch
        all_predictions_single_batch.append(average_predictions_single_image)

    # Convert the list of predictions for a single batch to a numpy array
    all_predictions_single_batch = np.array(all_predictions_single_batch)

    # Append the predictions for a single batch to the list for all images
    all_predictions_all_images.append(all_predictions_single_batch)

# Convert the list of predictions to a numpy array
all_predictions_all_images = np.vstack(all_predictions_all_images)

# Get the final predicted class for each image as the one with the highest average probability
predicted_classes_all_images = np.argmax(np.mean(all_predictions_all_images, axis=1), axis=1)

# Convert one-hot encoded labels to integers for comparison
true_labels_all_images = np.argmax(test_generator.labels, axis=1)

# Calculate accuracy
accuracy = accuracy_score(true_labels_all_images, predicted_classes_all_images)
print("Accuracy:", accuracy)

# Calculate test loss and accuracy based on the final averaged predictions
test_loss, test_accuracy = best_model.evaluate(test_generator)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")
